{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import RMSprop\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Base Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'C:/images/OX_images/'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# training dataset Path\n",
    "train_o_dir = os.path.join(train_dir, 'O')\n",
    "train_x_dir = os.path.join(train_dir, 'X')\n",
    "train_n_dir = os.path.join(train_dir, 'None')\n",
    "\n",
    "# validation dataset Path\n",
    "validation_o_dir = os.path.join(validation_dir, 'O')\n",
    "validation_x_dir = os.path.join(validation_dir, 'X')\n",
    "validation_n_dir = os.path.join(validation_dir, 'None')\n",
    "\n",
    "# test dataset Path\n",
    "test_o_dir = os.path.join(test_dir, 'O')\n",
    "test_x_dir = os.path.join(test_dir, 'X')\n",
    "test_n_dir = os.path.join(test_dir, 'None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image data Pre-processing\n",
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomRotation(25),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(150, scale=(0.8, 1.2), ratio=(0.75, 1.33)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5])\n",
    "])\n",
    "\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5])\n",
    "])\n",
    "\n",
    "test_transforms = validation_transforms\n",
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
    "validation_dataset = datasets.ImageFolder(validation_dir, transform=validation_transforms)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'None': 0, 'O': 1, 'X': 2}\n"
     ]
    }
   ],
   "source": [
    "class_indices = train_dataset.class_to_idx\n",
    "print(class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OX_Model_CNN(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=10368, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class OX_Model_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OX_Model_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, 1, 1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(32*18*18, 512)\n",
    "        self.fc2 = nn.Linear(512, 3)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = x.view(-1, 32*18*18)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = OX_Model_CNN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 3.425785097810957, Validation Loss: 1.0047202533797215, Validation Accuracy: 0.40789473684210525\n",
      "Epoch 2/30, Loss: 1.0159441696272955, Validation Loss: 0.9344194405957272, Validation Accuracy: 0.7192982466597306\n",
      "Epoch 3/30, Loss: 0.7526435057322184, Validation Loss: 0.6389174094227584, Validation Accuracy: 0.7631578947368421\n",
      "Epoch 4/30, Loss: 0.43152809143066406, Validation Loss: 0.47205568494054634, Validation Accuracy: 0.75\n",
      "Epoch 5/30, Loss: 0.31170423660013413, Validation Loss: 0.42184159505134294, Validation Accuracy: 0.7894736842105263\n",
      "Epoch 6/30, Loss: 0.21349244564771652, Validation Loss: 0.32956921800130085, Validation Accuracy: 0.9078947368421053\n",
      "Epoch 7/30, Loss: 0.21012482254041565, Validation Loss: 0.29410632730823155, Validation Accuracy: 0.9342105263157895\n",
      "Epoch 8/30, Loss: 0.13280315200487772, Validation Loss: 0.24599438724320216, Validation Accuracy: 0.9210526315789473\n",
      "Epoch 9/30, Loss: 0.09622875725229581, Validation Loss: 0.15155152910337896, Validation Accuracy: 0.9605263157894737\n",
      "Epoch 10/30, Loss: 0.12107822433527973, Validation Loss: 0.1848499140402813, Validation Accuracy: 0.9342105263157895\n",
      "Epoch 11/30, Loss: 0.05175585993048218, Validation Loss: 0.16672740484203744, Validation Accuracy: 0.9605263157894737\n",
      "Epoch 12/30, Loss: 0.03566953612284528, Validation Loss: 0.23290041167481795, Validation Accuracy: 0.9342105263157895\n",
      "Epoch 13/30, Loss: 0.04819591723692914, Validation Loss: 0.14445966123304513, Validation Accuracy: 0.9605263157894737\n",
      "Epoch 14/30, Loss: 0.04921693693742984, Validation Loss: 0.11853557022435343, Validation Accuracy: 0.9736842105263158\n",
      "Epoch 15/30, Loss: 0.023922622022736404, Validation Loss: 0.11719633136003811, Validation Accuracy: 0.9736842105263158\n",
      "Epoch 16/30, Loss: 0.010428016643143363, Validation Loss: 0.12408150125329005, Validation Accuracy: 0.9736842105263158\n",
      "Epoch 17/30, Loss: 0.017692489420167275, Validation Loss: 0.12285530686274701, Validation Accuracy: 0.9736842105263158\n",
      "Epoch 18/30, Loss: 0.1022372127044946, Validation Loss: 0.13120122164355053, Validation Accuracy: 0.9605263157894737\n",
      "Epoch 19/30, Loss: 0.06506323607431518, Validation Loss: 0.1158712975023938, Validation Accuracy: 0.9473684210526315\n",
      "Epoch 20/30, Loss: 0.14180791456924957, Validation Loss: 0.10136654975270319, Validation Accuracy: 0.9736842105263158\n",
      "Epoch 21/30, Loss: 0.04898643881703416, Validation Loss: 0.07886019498553914, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 22/30, Loss: 0.07045265714017053, Validation Loss: 0.04176557963707015, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 23/30, Loss: 0.03672107594967303, Validation Loss: 0.05017812020318053, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 24/30, Loss: 0.06492256181728509, Validation Loss: 0.06345619107753689, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 25/30, Loss: 0.012560020097427897, Validation Loss: 0.051652893178841246, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 26/30, Loss: 0.03522589018878838, Validation Loss: 0.04920860301432923, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 27/30, Loss: 0.015189435048442747, Validation Loss: 0.05307700255683301, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 28/30, Loss: 0.008512522320314828, Validation Loss: 0.03328915497274097, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 29/30, Loss: 0.1263644271530211, Validation Loss: 0.04088937310337159, Validation Accuracy: 0.9868421052631579\n",
      "Epoch 30/30, Loss: 0.035468765870771475, Validation Loss: 0.04995352171805877, Validation Accuracy: 0.9736842105263158\n"
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "epochs = 30\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    best_loss = 100.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            accuracy += (predicted == labels).float().mean().item()\n",
    "            if validation_loss < best_loss:\n",
    "                torch.save(model.state_dict(), 'OX_class_model.pth')\n",
    "                best_loss = validation_loss\n",
    "            \n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader)}, \"\n",
    "          f\"Validation Loss: {validation_loss / len(validation_loader)}, \"\n",
    "          f\"Validation Accuracy: {accuracy / len(validation_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0017295093624852599, Train Accuracy: 1.0\n",
      "Validation Loss: 0.36141845083620133, Validation Accuracy: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_accuracy += (predicted == labels).float().mean().item()\n",
    "            \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_accuracy = total_accuracy / len(loader)\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "train_loss, train_accuracy = evaluate_model(train_loader)\n",
    "print(f\"Train Loss: {train_loss}, Train Accuracy: {train_accuracy}\")\n",
    "\n",
    "validation_loss, validation_accuracy = evaluate_model(validation_loader)\n",
    "print(f\"Validation Loss: {validation_loss}, Validation Accuracy: {validation_accuracy}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'OX_class_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
